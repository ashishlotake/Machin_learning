{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70a9d362-a8e2-4a5b-bbc9-029e8d47c0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1afd21-8d46-4ebe-ab11-134ee81e343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"lsh_assignment_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a691f55-0349-42ee-91d8-4f8cf4d554c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing stop word \n",
    "\n",
    "import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "# print(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4db76fd-eea0-4938-9390-25fdc8f94493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into Trainng and Test data\n",
    "\n",
    "train_data = df.iloc[:-10,:]\n",
    "test_data = df.iloc[-10:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad63b244-6e14-4dfd-9d82-80a299f20d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf-vectorizer with bi-gram and tri-gram of words having min-frequency of 10 and only consider 4000 features\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,3), min_df=10, max_features=4000)#, stop_words=stopwords)\n",
    "n_grams = vectorizer.fit_transform(train_data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16df2d27-9358-4d24-8bf0-0c593ab0c506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2215, 4000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9bb90b6-9894-4cd1-9ef0-7dda80daf519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random planes \n",
    "# set the random seed to 0\n",
    "np.random.seed(0)\n",
    "\n",
    "def create_random_plane(a, dim):\n",
    "    w = []\n",
    "    for i in range(1,a+1):\n",
    "        w.append(np.random.normal(0,1,dim)) # the dimension of the each plane will be same as dimension as the each data point in train_data\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb69b6c1-4499-4009-8946-0f691557eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_planes = create_random_plane(5, n_grams.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddede061-872a-4063-bbb8-dbb315e8580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_val(vector, m_planes):\n",
    "    '''\n",
    "    This function return the hash values by computing the dot product of vector and m_planes\n",
    "    vector and m_planes belongs to d-dimension then hash values will \"d\" number of char\n",
    "    \n",
    "    # if (w.T).(v) < 0 --> 0\n",
    "    # if (w.T).(v) > 1 --> 1\n",
    "    '''\n",
    "    st = ''\n",
    "    for i in m_planes:\n",
    "        dot_pr = (vector.dot(i))  \n",
    "        ## as type(vector) -->  scipy.sparse.csr.csr_matrix\n",
    "        ## type(m_planes[i]) --> np.ndarray\n",
    "        if dot_pr < 0:\n",
    "            st += \"0\"\n",
    "        elif dot_pr >0:\n",
    "            st+=\"1\"\n",
    "    return st\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "246a1c30-f98a-45e0-be3d-6b60b57a7fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01101'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_val(vec1, m_planes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "024a3293-f019-4693-8de5-42105082a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_label = list(range(len(list(train_data[\"category\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d097115b-3770-4d62-aea0-502e041e997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_create_2(n_grams, vector_label, m_planes):\n",
    "    '''\n",
    "    input:\n",
    "            vectors ---> vector representation of each data point\n",
    "            vector_label --> vector name of corresponding vector-form in vectors\n",
    "            m_planes ---> these are the randomnly generated planes \n",
    "    \n",
    "    I will create a dict of the given vectors,and store them as hash_value:vectore_name\n",
    "    '''\n",
    "    hash_dict = {}\n",
    "    \n",
    "    for i in tqdm(range(len(vector_label))):\n",
    "        hash_value = hash_val(n_grams[i], m_planes)\n",
    "        hash_dict.setdefault(hash_value, []).extend([vector_label[i]])\n",
    "    return hash_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82cc18d7-fc33-4d63-a352-5b1297ff5fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2215/2215 [00:00<00:00, 10806.31it/s]\n"
     ]
    }
   ],
   "source": [
    "hash_dict = hash_create_2(n_grams, vec_label, m_planes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966c4dd-49b0-4ca6-bd3b-b0e1b2e7a320",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dedf7c1-7c17-4a14-b472-ff9125dec1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>NaN</td>\n",
       "      <td>junk e-mails on relentless rise spam traffic i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>NaN</td>\n",
       "      <td>top stars join us tsunami tv show brad pitt  r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rings of steel combat net attacks gambling is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>NaN</td>\n",
       "      <td>davies favours gloucester future wales hooker ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>NaN</td>\n",
       "      <td>beijingers fume over parking fees choking traf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>NaN</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>NaN</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>NaN</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                                               text\n",
       "2215      NaN  junk e-mails on relentless rise spam traffic i...\n",
       "2216      NaN  top stars join us tsunami tv show brad pitt  r...\n",
       "2217      NaN  rings of steel combat net attacks gambling is ...\n",
       "2218      NaN  davies favours gloucester future wales hooker ...\n",
       "2219      NaN  beijingers fume over parking fees choking traf...\n",
       "2220      NaN  cars pull down us retail figures us retail sal...\n",
       "2221      NaN  kilroy unveils immigration policy ex-chatshow ...\n",
       "2222      NaN  rem announce new glasgow concert us band rem h...\n",
       "2223      NaN  how political squabbles snowball it s become c...\n",
       "2224      NaN  souness delight at euro progress boss graeme s..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6115c4c8-2f73-4309-b0f0-2cd9052fe027",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gram = vectorizer.transform(test_data.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97025576-b584-4659-adc2-a80536d08da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vector1, vector2):\n",
    "    '''\n",
    "    The function return the cosime similarity between vector1 and vector2\n",
    "    '''\n",
    "    return (vector2.dot(vector1))/(np.linalg.norm(vector1)* np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97ddbeff-ab5c-4ecb-ad65-54491749c476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01110',\n",
       " '10110',\n",
       " '01001',\n",
       " '01101',\n",
       " '10110',\n",
       " '00110',\n",
       " '01010',\n",
       " '10011',\n",
       " '10001',\n",
       " '10010']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hash = []\n",
    "for i in test_gram:\n",
    "    test_hash.append(hash_val(i, m_planes))\n",
    "\n",
    "test_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ff149e4-ba6d-4e85-9d9e-23e2f15b496a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 96, 105, 153, 239, 243, 315, 401, 405, 468, 511, 587, 680, 694, 771, 780, 809, 847, 855, 876, 963, 973, 1029, 1113, 1175, 1223, 1224, 1378, 1408, 1416, 1461, 1481, 1576, 1665, 1691, 1899, 1930, 2022, 2037, 2171, 2213]\n"
     ]
    }
   ],
   "source": [
    "new = []\n",
    "# count = 0\n",
    "for i in test_gram:\n",
    "    new.append([x for x in hash_dict[hash_val(i, m_planes)]])\n",
    "\n",
    "    \n",
    "print(new[0]) \n",
    "## this dict contain all the vector_name that have same hash_values as the query_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f4e3a9d-1b6f-494d-a19e-c6572c04fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vect = n_grams.toarray() # convert the n_gram(spare csr.matrix) to numpy.ndarray, for easy calculation\n",
    "\n",
    "\n",
    "knn_11 = []\n",
    "for i in range(0,len(new)):\n",
    "    cosine_similarity = {}\n",
    "    for j in range(len(train_vect[new[i]])):\n",
    "        cos_sim = cosine_sim(test_gram[i], train_vect[new[i][j]]) \n",
    "        ## computing the cosine-sim between all the querry_points with the training points(which have same hash_value as the querry point)\n",
    "        cosine_similarity[new[i][j]] = cos_sim \n",
    "        ## then store the value in dict PT_NAME:cosine-sim(between xq and PT_NAME)\n",
    "    knn_11.append(cosine_similarity) \n",
    "    ## Finally append all the dicts in the list, which contain consine-sim between xq and the pts which have same hash_value as xq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8465d0b3-cb7b-4475-bd53-9b69c25436b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0000000000000002, 963),\n",
       " (0.11124613300644823, 1416),\n",
       " (0.10942722899280966, 239),\n",
       " (0.09309289878365656, 2213),\n",
       " (0.09309289878365656, 401),\n",
       " (0.08702680027698745, 2022),\n",
       " (0.06702712530502797, 315),\n",
       " (0.05782779241516406, 105),\n",
       " (0.056929721379619776, 2037),\n",
       " (0.04513601479561055, 1408)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = sorted(((value, key) for (key,value) in knn_11[0].items()), reverse=True)[:10]\n",
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e2c46a7-b98b-48c9-98b5-5f4d1962cca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[963, 1416, 239, 2213, 401, 2022, 315, 105, 2037, 1408]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_f = list(v for k,v in cc)\n",
    "c_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb49da72-4f17-45d8-912b-160667298e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    " \n",
    "def most_frequent(List):\n",
    "    occurence_count = Counter(List)\n",
    "    return occurence_count.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "## source -- https://www.geeksforgeeks.org/python-find-most-frequent-element-in-a-list/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f131071b-fea6-4bea-800d-f6503e621cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tech', 'entertainment', 'tech', 'tech', 'business', 'business', 'politics', 'entertainment', 'politics', 'sport']\n"
     ]
    }
   ],
   "source": [
    "pred_class_labels = []\n",
    "for i in range(len(knn_11)):\n",
    "    ## this line sort the dict (which contain all the pts which have hash_value same as query point along with their cosine-sim between xq and that point)\n",
    "    ## the output of this is list, which have point in descending order of cosine-sim (between xq and pt)\n",
    "    top_11_nss = (list(v for k,v in (sorted(((value, key) for (key,value) in knn_11[i].items()), reverse=True)[:10])))\n",
    "    # print(top_11_nss)\n",
    "    \n",
    "    class_label = [ train_data.category.iloc[i] for i in top_11_nss] ## find all the labels of the top 11-NN for each querry point\n",
    "    # print(class_label)\n",
    "    pred_class_labels.append(most_frequent(class_label))\n",
    "    \n",
    "    \n",
    "print(pred_class_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3518c322-7135-48e5-86c4-069735239e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tech', 'entertainment', 'tech', 'sport', 'business', 'business', 'politics', 'entertainment', 'politics', 'sport']\n"
     ]
    }
   ],
   "source": [
    "x= ['tech', 'entertainment', 'tech', 'sport', 'business', 'business', 'politics', 'entertainment', 'politics', 'sport']\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb8ffc-9215-4597-b98e-70ce841d274b",
   "metadata": {},
   "source": [
    "## Readings/References \n",
    "\n",
    "    https://www.pinecone.io/learn/locality-sensitive-hashing-random-projection/\n",
    "    https://santhoshhari.github.io/Locality-Sensitive-Hashing/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc4aac9-4895-46c9-8ff6-1e7f37308f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "## GRADER CELL: Do NOT Change this.\n",
    "# This cell will print \"Success\" if your implmentation of the predictLabels() is correct and the accuracy obtained is above 80%.\n",
    "# Else, it will print \"Failed\"\n",
    "###########################################\n",
    "import numpy as np\n",
    "\n",
    "# Predict the labels using the predictLabels() function\n",
    "Y_custom = np.array(predictLabels(test_data))\n",
    "\n",
    "# Reference grader array - DO NOT MODIFY IT\n",
    "Y_grader = np.array(['tech', 'entertainment', 'tech', 'sport', 'business', 'business', 'politics', 'entertainment', 'politics', 'sport'])\n",
    "\n",
    "# Calculating accuracy by comparing Y_grader and Y_custom\n",
    "accuracy = np.sum(Y_grader==Y_custom) * 10\n",
    "\n",
    "if accuracy >= 80:\n",
    "    print(\"******** Success ********\",\"Accuracy Achieved = \", accuracy,'%')\n",
    "else:\n",
    "    print(\"####### Failed #######\",\"Accuracy Achieved = \", accuracy,'%')\n",
    "    print(\"\\nY_grader = \\n\\n\", Y_grader)\n",
    "    print(\"\\n\",\"*\"*50)\n",
    "    print(\"\\nY_custom = \\n\\n\", Y_custom)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
